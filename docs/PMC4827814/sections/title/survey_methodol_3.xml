<?xml version="1.0" encoding="UTF-8"?>
<sec id="sec003" class="sec">
 <div class="title" xmlns="http://www.w3.org/1999/xhtml">Survey Methodology</div>
 <p xmlns="http://www.w3.org/1999/xhtml">The survey responses were collected through e-mail by Marketing Systems Group (MSG) and SurveySavvy, a professional survey company. The baseline survey was conducted between April 9 and April 17, 2013. A follow-up survey was conducted 6 months later, between September 30 and October 28, 2013. Subjects were recruited from SurveySavvy’s current pool of survey respondents. SurveySavvy recruits subjects into the survey pool through the company’s website and a proprietary system of online referrals. In addition, SurveySavvy reaches out to groups that are under-represented in their pool via e-mail and telephone, both landline and cellular, in order to ensure that the survey pool is close to nationally representative. Recruitment information screen shots, the text of the e-mail sent to the prospective participants, and a recruitment flowchart can be found in 
  <a ref-type="supplementary-material" rid="pone.0151469.s001" href="#pone.0151469.s001">S1 Text</a>.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">The study received approval from the University of Illinois, Urbana-Champaign’s Institutional Review Board for non-biomedical human subject research. The only criterion for participating in the baseline survey was for respondents to be 18 years of age or older (self-reported) and located in the United States. An additional criterion for the 6-month follow-up survey was for respondents to have completed the baseline survey. No vulnerable populations were targeted, and none of the subjects were likely to be associated with the researcher. All participants read an informed consent form prior to starting the survey. This form informed the participants about the purpose of the study, the duration of the survey (approximately 10 minutes), the incentive for survey completion (the opportunity to win one of six $50 prizes in addition to a financial incentive provided by SurveySavvy), the existence of a 6-months follow-up survey, the fact that participation was completely voluntary and could be terminated at any point, and the anonymity and processing of individual data. The form also provided investigator contact information. Subjects were not exposed to any risks or deception and were able to quit the survey at any point. At the end of the form, the participants could either terminate participation or click “Yes” to continue to the survey. Due to the online nature of the survey and minimal risk to the subjects, IRB granted a waiver of written informed consent. The full informed consent forms for the initial survey and 6-month follow-up are available in 
  <a ref-type="supplementary-material" rid="pone.0151469.s002" href="#pone.0151469.s002">S2 Text</a>. The survey did not ask for personally identifiable information, such as name or email. In order to ensure complete anonymity, results are presented in the aggregate across all survey respondents. Finally, the data are stored on the hard drives of the researchers’ password-protected computers.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">The “closed” electronic survey was sent to prospective participants via e-mail that provided a unique link to the survey. The survey was programmed online using SurveyMonkey.com. The uniqueness of the link allowed duplicate responses to be identified and dropped. Thus, it was not necessary to use cookies to track respondents. Similar questions were grouped on the same screen. There were a total of 14 screens in the baseline survey: a screen with the informed consent information, 12 question screens, and a screen thanking respondents for their participation. There were a total of 13 screens in the 6-month follow-up survey: a screen with the informed consent information, 11 question screens, and a screen thanking respondents for their participation. Participants could not go back to review or change their answers to previous questions in either survey. The order of questions was kept the same across all participants, and adaptive questioning was not applicable. The order of answers within each multiple choice question was randomized, but the order of the questions themselves did not vary. The full survey text is available in 
  <a ref-type="supplementary-material" rid="pone.0151469.s003" href="#pone.0151469.s003">S3 Text</a>. Prior to running the full-scale survey, the usability and technical functionality of the electronic questionnaire was tested with 5 volunteers. A pilot survey with 201 respondents was conducted to test whether questions were well-formulated. Because there was no follow-up with the pilot respondents, they are excluded from the results reported in this paper. However, their inclusion would not significantly change the baseline results.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">Out of the 2,484 invited participants, 1,593 respondents initiated and 1,300 completed the baseline survey (for a participation rate of 64% and a completion rate of 82%). Because the survey was sent to prospective participants via e-mail, the “view rate” is equal to the participation rate in our survey; we did not have the capability to track whether respondents opened the email or not. Out of the 1,300 participants who completed the baseline survey, 886 initiated and 747 completed the follow-up survey (for a participation rate of 68% and a completion rate of 84%). While there were some statistically significant differences between the characteristics of the initial sample and of those who completed the follow-up survey (see 
  <a ref-type="supplementary-material" rid="pone.0151469.s004" href="#pone.0151469.s004">S4 Text</a>), there was no differential attrition by treatment status.
 </p>
 <p xmlns="http://www.w3.org/1999/xhtml">Only completed questionnaires were analyzed. We did not measure the time respondents spent answering the questions; all completed questionnaires were used in our analysis. No statistical correction methods were applied to either the baseline or the follow-up samples. Summary statistics of respondent characteristics can be found in 
  <a ref-type="supplementary-material" rid="pone.0151469.s004" href="#pone.0151469.s004">S4 Text</a>. Our sample was broadly similar to the US as a whole, with the exception of education: a significantly higher share of our respondents had a bachelor’s degree or higher [
  <a rid="pone.0151469.ref018" ref-type="bibr" href="#pone.0151469.ref018">18</a>].
 </p>
</sec>
